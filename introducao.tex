\chapter{INTRODUÇÃO}
\thispagestyle{empty}

\section{Introdução do tema}
As redes neurais artificiais (do inglês, \textit{Artificial Neural Network} - ANN) são utilizadas em diversas aplicações para resolução de problemas, como visão computacional, reconhecimento de fala e de padrões \cite{applications}. Para que esses algoritmos alcancem um resultado satisfatório, um dos principais requisitos é realizar o treinamento da rede com o uso de um conjunto de dados que representem bem os casos do mundo real, com o objetivo de garantir a generalização do aprendizado para dados não vistos.

Apesar do grande sucesso das redes neurais, elas podem ser classificadas como um algoritmo caixa preta, sendo, basicamente, o funcionamento computado para a aquisição da resposta da ANN, dado um conjunto de entradas, não humanamente interpretável. Essa falta de explicações para o seu comportamento pode prejudicar sistemas críticos que permitem apenas uma pequena margem para falhas, como aplicações médicas e financeiras. Além disso, com o avanço de leis de proteção de dados observável em diversas regiões do mundo, como na União Europeia \cite{lei_EU}, está ficando cada vez mais necessário disponibilizar uma explicação do que está sendo realizado com os dados dos seus respectivos donos. 

A existência de exemplos adversariais é outro motivo para a necessidade do desenvolvimento de abordagens que computam explicações para algoritmos de aprendizagem de máquina, incluindo redes neurais. De acordo com o trabalho de \citeonline{adversarial_examples}, exemplos adversariais são instâncias classificadas erroneamente por um modelo de aprendizagem de máquina que são minimamente diferentes de uma outra instância classificada corretamente, demonstrando a fraqueza desses algoritmos para pequenas pertubações nos dados de entrada que, muitas vezes, poderiam terem sido adquiridas na própria etapa de aquisição do conjunto de dados. 

Os métodos heurísticos são as principais abordagens utilizadas com o intuito de disponibilizar explicações para as saídas de qualquer modelo de aprendizagem de máquina atualmente, pelo fato de serem considerados algoritmos \textit{model-agnostic}, como o LIME \cite{LIME} e ANCHOR \cite{ANCHOR}. Porém, de acordo com o trabalho de \citeonline{Ignatiev_why}, como eles exploram o espaço da instância localmente, esses algoritmos não acarretam em respostas que possuam garantias formais, resultando em explicações locais que podem ser otimistas (quando existem instâncias no espaço de instâncias que a explicação computada falha) ou pessimistas (quando alguma literal pertencente à explicação computada é irrelevante e pode ser descartada).t

Assim, abordagens não heurísticas para a computação dessas explicações tem sido abordadas em alguns trabalhos recentes. O artigo de \citeonline{Ignatiev_abduction} apresenta  um algoritmo \textit{model-agnostic}, assim como os métodos heurísticos, porém a computação das explicações mínimas geradas carregam uma garantia formal, pois a sua proposta é baseada em lógica utilizando o raciocínio abdutivo. A abordagem basicamente codifica um modelo $M$ de aprendizagem de máquina como um conjunto de restrições lineares $F$ em alguma teoria, a partir de um verificador de rede neural, que no caso é usado a modelagem de um problema de Programação Linear Inteira Mista (do inglês, \textit{Mixed Integer Linear Programming} - MILP) proposto por \citeonline{milp_01}, então, dado uma instância $C$ associado com a predição $E$ tal que $C \wedge F \models E$ (equivalente à $C \models (F \rightarrow E)$), é computada a explicação mínima $C'$ (dado $C$) que é implicante principal de  $F \rightarrow E$.

Fundamentado na importância de disponibilizar explicações decorrentes de algoritmos que carregam garantias formais, este trabalho possui o objetivo de apresentar uma abordagem similar ao artigo de \citeonline{Ignatiev_abduction}, utilizando o raciocínio abdutivo para a aquisição de explicações mínimas para redes neurais artificiais e comparando os resultados gerados usando diferentes codificadores para os modelos testados.

\section{Resumo dos artigos selecionados}

O trabalho de \citeonline{Ignatiev_abduction} apresenta um algoritmo \textit{model-agnostic}, significando que funciona em qualquer modelo desde que ele possa ser representado por um sistema de raciocínio de restrição e que consultas de implicação possam ser decididas por um oráculo. Essa abordagem utiliza o raciocínio abdutivo para computar explicações mínimas para modelos de aprendizagem de máquina com garantias formais, fornecendo explicações de cardinalidade mínima ou subconjunto mínima. A abordagem basicamente codifica um modelo $M$ de aprendizagem de máquina como um conjunto de restrições lineares $F$ em alguma teoria, a partir de um algoritmo 0-1 MILP \cite{milp_01}, então, dado uma instância $C$ associado com a predição $E$ tal que $C \wedge F \models E$ (equivalente à $C \models (F \rightarrow E)$), é computada a explicação mínima $C'$ (dado $C$) que é implicante principal de  $F \rightarrow E$. 

O artigo de \citeonline{milp_01} propõe um algoritmo que realiza a codificação de uma rede neural profunda como um 0-1 MILP que usa restrições de indicação para evitar o uso da notação de \textit{Big-M}, utilizando variáveis de ativação binárias para impor as implicações lógicas. Foi realizado a modelagem de aplicações que usavam redes com ReLUs e \textit{max/average pooling}. Esse modelo codificado em restrições lineares não é suscetível à treinamento, pois ele se torna bilinear nessa configuração. Foi realizado experimentos do algoritmo em dois problemas de aprendizagem de máquina: visualização de características e aprendizagem de máquina adversário.

O trabalho de \citeonline{milp_top} implementa um algoritmo MILP que codifica as partes lineares (camadas que usam transformações lineares ou funções que possuam partes lineares, como ReLU e \textit{max pooling}) de uma rede neural como restrições lineares, minimizando o número de variáveis binárias presentes no problema MILP e melhorando o condicionamento numérico. Seja a função de ativação ReLU $y = max(x, 0)$ e $l \leq x \leq u$, para a sua formulação é considerado que há 3 fases: a unidade está estavelmente inativa (quando $u \leq 0$), estavelmente ativa (quando $l \geq 0$) e instável (caso contrário). Para unidades instáveis é utilizado o conjunto de restrições lineares e inteiras na Equação \ref{eq:milp_top}, com a introdução de uma variável de indicação de decisão $a = 1_{x \geq 0}$.

\begin{equation}
    (y \leq x - l(1- a)) \wedge (y \geq x) \wedge (y \leq u*a) \wedge (y \geq 0) \wedge (a \in \{0, 1\})
    \label{eq:milp_top}
\end{equation}

Para indicar se uma unidade ReLU é instável ou não, é preciso definir os seus limites de entrada o mais achatado possível, porém ainda sendo válidos. Para isso, são utilizados dois procedimentos: Intervalo Aritmético e Programação Linear. Essa abordagem foi realizada na aplicação de exemplos adversariais sendo de duas a três vezes de ordem de magnitude mais rápida que o estado da arte, permitindo um aumento significativo do tamanho das redes neurais codificadas.

O artigo de \citeonline{Ignatiev_why} descreve os experimentos realizados que questionam as explicações de modelos de aprendizagem de máquina dadas por métodos heurísticos, que computam uma explicação explorando localmente o espaço da instância. Para realização desses experimentos é utilizado abordagens baseadas em lógica com o cálculo de implicantes principais por meio do raciocínio abdutivo, realizando os testes em \textit{boosted trees}. Os algoritmos desenvolvidos possuem os objetivos de acessar a qualidade das explicações locais, reparar as explicações locais que são otimistas (quando existem instâncias no espaço de instâncias que a explicação computada falha) e refinar as explicações locais que são pessimistas (quando alguma literal pertencente à explicação computada é irrelevante e pode ser descartada).

O trabalho de \citeonline{reluplex} implementa o algoritmo Reluplex, um SMT \textit{solver} para a teoria da aritmética linear real que tem o objetivo de verificar redes neurais profundas utilizando uma técnica baseada no Simplex, porém adaptada para lidar com a função de ativação não-convexa ReLU sem simplificações no seu funcionamento original, apenas permitindo que suas entradas e saídas sejam temporariamente inconsistentes e corrigidas conforme a execução do algoritmo. O Reluplex foi avaliado em 45 redes neurais profundas desenvolvidas como um protótipo inicial do sistema anti-colisão de última geração para aeronaves não tripuladas ACAS Xu. 

Este trabalho é fundamentado no artigo de \citeonline{Ignatiev_abduction}, que utilizada um 0-1 MILP \cite{milp_01} para a codificação da rede neural como um conjunto de restrições lineares em uma teoria, então são computadas explicações para os seus resultados com o uso do raciocínio abdutivo. A necessidade de tais explicações computadas carregando uma garantia formal é evidenciada em \citeonline{Ignatiev_why}. Um dos objetivos iniciais é comparar os possíveis resultados desse algoritmo utilizando diferentes MILPs e estratégias de codificação (verificação) das redes neurais, como os implementados em \citeonline{milp_top} e \citeonline{reluplex}.