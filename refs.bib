@article{Ignatiev_abduction, 
title={Abduction-Based Explanations for Machine Learning Models}, 
volume={33}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/3964}, DOI={10.1609/aaai.v33i01.33011511}, 
abstractNote={&lt;p&gt;The growing range of applications of Machine Learning (ML) in a multitude of settings motivates the ability of computing small explanations for predictions made. Small explanations are generally accepted as easier for human decision makers to understand. Most earlier work on computing explanations is based on heuristic approaches, providing no guarantees of quality, in terms of how close such solutions are from cardinality- or subset-minimal explanations. This paper develops a constraint-agnostic solution for computing explanations for any ML model. The proposed solution exploits abductive reasoning, and imposes the requirement that the ML model can be represented as sets of constraints using some target constraint reasoning system for which the decision problem can be answered with some oracle. The experimental results, obtained on well-known datasets, validate the scalability of the proposed approach as well as the quality of the computed solutions.&lt;/p&gt;}, number={01}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Ignatiev, Alexey and Narodytska, Nina and Marques-Silva, Joao}, 
year={2019-a}, 
month={Jul.}, 
pages={1511-1519} 
}

@article{milp_01,
author = {Fischetti, Matteo and Jo, Jason},
year = {2018},
month = {Jul.},
pages = {296-309},
title = {Deep neural networks and mixed integer linear optimization},
volume = {23},
journal = {Constraints},
doi = {10.1007/s10601-018-9285-6},
url={https://link.springer.com/article/10.1007/s10601-018-9285-6},
}

@inproceedings{milp_top,
title={Evaluating Robustness of Neural Networks with Mixed Integer Programming},
author={Vincent Tjeng and Kai Y. Xiao and Russ Tedrake},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HyGIdiRqtm},
}

@article{Ignatiev_why,
  author    = {Alexey Ignatiev and
               Nina Narodytska and
               Jo{\~{a}}o Marques{-}Silva},
  title     = {On Validating, Repairing and Refining Heuristic {ML} Explanations},
  journal   = {CoRR},
  volume    = {abs/1907.02509},
  year      = {2019-b},
  url       = {http://arxiv.org/abs/1907.02509},
  archivePrefix = {arXiv},
  eprint    = {1907.02509},
  timestamp = {Mon, 24 Feb 2020 19:23:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-02509.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{reluplex,
	doi = {10.1007/978-3-319-63387-9_5},
	url = {https://doi.org/10.1007\%2F978-3-319-63387-9_5},
	year = 2017,
	publisher = {Springer International Publishing},
	pages = {97--117},
	author = {Guy Katz and Clark Barrett and David L. Dill and Kyle Julian and Mykel J. Kochenderfer},
	title = {Reluplex: An Efficient {SMT} Solver for Verifying Deep Neural Networks},
	booktitle = {Computer Aided Verification}
}

@online{lei_EU,
    title = {Reforma de 2018 das regras de proteç{\~a}o de dados da UE},
    url = {https://ec.europa.eu/commission/sites/beta-political/files/data-protection-factsheet-changes_en.pdf},
    organization = {Comiss{\~a}o Europeia},
    year = 2018,
    urldate = {2021-05-14}
}

@inproceedings{LIME,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135–1144},
numpages = {10},
keywords = {interpretability, black box classifier, interpretable machine learning, explaining machine learning},
location = {São Francisco, Calif{\´o}rnia, EUA},
series = {KDD '16}
}

@inproceedings{ANCHOR,
  author    = {Marco T{\'{u}}lio Ribeiro and
               Sameer Singh and
               Carlos Guestrin},
  editor    = {Sheila A. McIlraith and
               Kilian Q. Weinberger},
  title     = {Anchors: High-Precision Model-Agnostic Explanations},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence,
               (AAAI-18), the 30th innovative Applications of Artificial Intelligence
               (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in
               Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
               2-7, 2018},
  pages     = {1527--1535},
  publisher = {{AAAI} Press},
  year      = {2018},
  url       = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16982}
}

@article{applications,
title = {A survey of deep neural network architectures and their applications},
journal = {Neurocomputing},
volume = {234},
pages = {11-26},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.12.038},
url = {https://www.sciencedirect.com/science/article/pii/S0925231216315533},
author = {Weibo Liu and Zidong Wang and Xiaohui Liu and Nianyin Zeng and Yurong Liu and Fuad E. Alsaadi},
keywords = {Autoencoder, Convolutional neural network, Deep learning, Deep belief network, Restricted Boltzmann machine},
abstract = {Since the proposal of a fast learning algorithm for deep belief networks in 2006, the deep learning techniques have drawn ever-increasing research interests because of their inherent capability of overcoming the drawback of traditional algorithms dependent on hand-designed features. Deep learning approaches have also been found to be suitable for big data analysis with successful applications to computer vision, pattern recognition, speech recognition, natural language processing, and recommendation systems. In this paper, we discuss some widely-used deep learning architectures and their practical applications. An up-to-date overview is provided on four deep learning architectures, namely, autoencoder, convolutional neural network, deep belief network, and restricted Boltzmann machine. Different types of deep neural networks are surveyed and recent progresses are summarized. Applications of deep learning techniques on some selected areas (speech recognition, pattern recognition and computer vision) are highlighted. A list of future research topics are finally given with clear justifications.}
}